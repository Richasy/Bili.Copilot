import{_ as t}from"./chunks/azure-oai-secret.BDDarwfJ.js";import{_ as a,c as r,a0 as o,o as i}from"./chunks/framework.CoDf8mVe.js";const n="/assets/azure-model-deploy.D-n33OKG.png",l="/assets/chat-custom-model.D0N2Z0Io.png",p="/assets/qianfan-model-hover.B6gmIoIa.png",d="/assets/chat-custom-model-wenxin.CHlzMtk3.png",h="/assets/chat-custom-model-ollama.B_-3z68v.png",I=JSON.parse('{"title":"对话服务配置","description":"","frontmatter":{},"headers":[],"relativePath":"chat-config.md","filePath":"chat-config.md","lastUpdated":null}'),s={name:"chat-config.md"};function c(b,e,m,f,k,u){return i(),r("div",null,e[0]||(e[0]=[o('<h1 id="对话服务配置" tabindex="-1">对话服务配置 <a class="header-anchor" href="#对话服务配置" aria-label="Permalink to &quot;对话服务配置&quot;">​</a></h1><p>哔哩助理支持多种主流 AI 对话服务，这些服务配置各异，这篇文档会对所支持的 AI 服务如何配置进行说明。</p><p>请根据子标题索引，跳转到你想要配置的服务段落查看。</p><h2 id="open-ai" tabindex="-1">Open AI <a class="header-anchor" href="#open-ai" aria-label="Permalink to &quot;Open AI&quot;">​</a></h2><table tabindex="0"><thead><tr><th>文档</th><th><a href="https://platform.openai.com/docs/api-reference/chat/create" target="_blank" rel="noreferrer">https://platform.openai.com/docs/api-reference/chat/create</a></th></tr></thead><tbody><tr><td>API 令牌</td><td><a href="https://platform.openai.com/account/api-keys" target="_blank" rel="noreferrer">https://platform.openai.com/account/api-keys</a></td></tr></tbody></table><p>Open AI 的配置相对来说很简单，只要你将 API Key 填入设置中的 <code>访问密钥</code> 即可。</p><h3 id="代理与-api-兼容" tabindex="-1">代理与 API 兼容 <a class="header-anchor" href="#代理与-api-兼容" aria-label="Permalink to &quot;代理与 API 兼容&quot;">​</a></h3><p>现在 Open AI 的接口数据结构几乎成为了一种通用标准，有很多的 AI 服务在暴露接口时为了更易用，都会选择以 Open AI 相似的接口地址和数据结构发布。这就是所谓的 <code>Open AI 兼容接口</code>。</p><p>如果你正在使用的 AI 服务不在哔哩助理的支持列表中，但又恰好使用了 Open AI 兼容接口，那么你就可以把该服务的地址写入 Open AI 设置区块的 <code>终结点（API）</code> 文本框中。</p><p>当然，还有另一种使用场景。</p><p>由于 Open AI 在部分国家或地区不可用，用户会尝试以代理服务器的方式使用 Open AI 服务，比如 <a href="https://www.openai-proxy.com/" target="_blank" rel="noreferrer">Open AI API Proxy</a>。</p><p>你依然需要使用自己的 API 密钥，但不直接访问 OpenAI 服务器，而是交由代理服务进行访问，从而避免被墙或者封号。</p><p>你可以将对应的服务地址填入 <code>终结点（API）</code> 中，以实现 API 代理功能。比如上面的代理服务，需要填入的地址就是：<code>https://api.openai-proxy.com/v1</code></p><div class="warning custom-block github-alert"><p class="custom-block-title">WARNING</p><p>上文提到的 Open AI API Proxy 仅用作说明该使用场景，开发者不对其安全性负责，使用者需要自行辨别服务的可靠性并为之负责。</p></div><div class="tip custom-block github-alert"><p class="custom-block-title">TIP</p><p>对于代理来说，版本号通常是有必要的，比如 <code>https://api.openai-proxy.com/v1</code> 中的 <code>v1</code>。 因为 API 输入框还需要兼容其它 API 服务，有些服务并不见得会在请求路径中使用版本号。</p></div><h2 id="azure-open-ai" tabindex="-1">Azure Open AI <a class="header-anchor" href="#azure-open-ai" aria-label="Permalink to &quot;Azure Open AI&quot;">​</a></h2><table tabindex="0"><thead><tr><th>文档</th><th><a href="https://learn.microsoft.com/azure/ai-services/openai/overview" target="_blank" rel="noreferrer">https://learn.microsoft.com/azure/ai-services/openai/overview</a></th></tr></thead><tbody><tr><td>Azure 服务</td><td><a href="https://azure.microsoft.com/en-us/products/ai-services/openai-service" target="_blank" rel="noreferrer">https://azure.microsoft.com/en-us/products/ai-services/openai-service</a></td></tr></tbody></table><p>Microsoft 与 Open AI 的关系较为特殊，有种 Open AI 特许经销商的感觉，所以你能够在 Azure 这个云服务平台创建 Open AI 资源，并按需部署 <code>GPT 3.5</code> , <code>GPT 4o</code> 等 Open AI 模型。</p><p>虽然它们的模型一致，但是网络请求格式却不尽相同，所需要的配置项也不一样。</p><p>当你部署了 Open AI 资源后，你能在资源页面的 <code>资源管理</code> -&gt; <code>密钥和终结点</code> 中找到所需的密钥（两个密钥任选其一即可）与终结点。</p><div style="max-width:500px;"><p><img src="'+t+'" alt="密钥与终结点"></p></div><p>将对应的值填入应用设置中。</p><p>你可能会对 API 版本感到好奇，这里是 <a href="https://learn.microsoft.com/azure/ai-services/openai/reference#completions" target="_blank" rel="noreferrer">文档</a>，通常情况下不需要改动。</p><h3 id="添加自定义模型" tabindex="-1">添加自定义模型 <a class="header-anchor" href="#添加自定义模型" aria-label="Permalink to &quot;添加自定义模型&quot;">​</a></h3><p>Azure Open AI 相较于 Open AI，最大的不同在于你需要手动部署需要的模型。</p><p>在应用中，Azure Open AI 不提供预置模型，仅仅填写密钥和终结点并不能让配置生效，我们还需要创建自定义模型。</p><p>在 <a href="https://" target="_blank" rel="noreferrer">Azure OpenAI Studio</a> 中，你可以从模型库里部署特定的模型。</p><p>每个模型有自己的 Id，比如 GPT 3.5 Turbo 的 Id 就是 gpt-35-turbo。</p><div style="max-width:420px;"><p><img src="'+n+'" alt="Azure 模型部署"></p></div><p>在你部署模型时，Azure 会要求你提供 <strong>部署名</strong>，这个很重要，我们通过 API 与服务进行交互式，模型的标识符不是模型 ID，而是你部署模型时的部署名。</p><p>我推荐在部署模型时尽量使用和模型 ID 相同的部署名，以减少歧义。但也存在例外情况，比如你部署了多个 <code>gpt 3.5 turbo</code>，但是它们的模型版本不同，那么你的部署名可能会是 <code>gpt-35-turbo-1106</code> 或 <code>gpt-35-turbo-0613</code>，那么在应用中创建自定义模型时，你可以按下图所示创建：</p><div style="max-width:300px;"><p><img src="'+l+'" alt="自定义聊天模型"></p></div><h2 id="gemini" tabindex="-1">Gemini <a class="header-anchor" href="#gemini" aria-label="Permalink to &quot;Gemini&quot;">​</a></h2><table tabindex="0"><thead><tr><th>文档</th><th><a href="https://ai.google.dev/gemini-api/docs" target="_blank" rel="noreferrer">https://ai.google.dev/gemini-api/docs</a></th></tr></thead><tbody><tr><td>API 令牌</td><td><a href="https://ai.google.dev/gemini-api/docs/api-key" target="_blank" rel="noreferrer">https://ai.google.dev/gemini-api/docs/api-key</a></td></tr></tbody></table><p>和 <a href="#open-ai">Open AI</a> 类似，获取访问密钥后，将其填入 <code>访问密钥</code> 中即可。</p><h3 id="代理" tabindex="-1">代理 <a class="header-anchor" href="#代理" aria-label="Permalink to &quot;代理&quot;">​</a></h3><p>Gemini 作为谷歌提供的 AI 服务，国内用户如果要访问，也可能需要代理，这一点和 <a href="#open-ai">OpenAI</a> 是一致的，请参考 Open AI 的配置方式，配置 Gemini 的代理服务。需要注意的是，Gemini 中的代理终结点不要加版本号，目前应用使用 <code>v1-beta</code>。</p><h2 id="anthropic" tabindex="-1">Anthropic <a class="header-anchor" href="#anthropic" aria-label="Permalink to &quot;Anthropic&quot;">​</a></h2><table tabindex="0"><thead><tr><th>文档</th><th><a href="https://docs.anthropic.com/zh-CN/docs/intro-to-claude" target="_blank" rel="noreferrer">https://docs.anthropic.com/zh-CN/docs/intro-to-claude</a></th></tr></thead><tbody><tr><td>API 令牌</td><td><a href="https://console.anthropic.com/account/keys" target="_blank" rel="noreferrer">https://console.anthropic.com/account/keys</a></td></tr></tbody></table><p>和 <a href="#open-ai">Open AI</a> 类似，获取访问密钥后，将其填入 <code>访问密钥</code> 中即可。</p><h2 id="月之暗面" tabindex="-1">月之暗面 <a class="header-anchor" href="#月之暗面" aria-label="Permalink to &quot;月之暗面&quot;">​</a></h2><p>即 Moonshot。</p><table tabindex="0"><thead><tr><th>文档</th><th><a href="https://platform.moonshot.cn/docs/api/chat#api-%E8%AF%B4%E6%98%8E" target="_blank" rel="noreferrer">https://platform.moonshot.cn/docs/api/chat#api-说明</a></th></tr></thead><tbody><tr><td>API 令牌</td><td><a href="https://platform.moonshot.cn/console/api-keys" target="_blank" rel="noreferrer">https://platform.moonshot.cn/console/api-keys</a></td></tr></tbody></table><p>和 <a href="#open-ai">Open AI</a> 类似，获取访问密钥后，将其填入 <code>访问密钥</code> 中即可。</p><h2 id="智谱-ai" tabindex="-1">智谱 AI <a class="header-anchor" href="#智谱-ai" aria-label="Permalink to &quot;智谱 AI&quot;">​</a></h2><p>ChatGLM 即出于此。</p><table tabindex="0"><thead><tr><th>文档</th><th><a href="https://open.bigmodel.cn/dev/howuse/introduction" target="_blank" rel="noreferrer">https://open.bigmodel.cn/dev/howuse/introduction</a></th></tr></thead><tbody><tr><td>API 令牌</td><td><a href="https://open.bigmodel.cn/usercenter/apikeys" target="_blank" rel="noreferrer">https://open.bigmodel.cn/usercenter/apikeys</a></td></tr></tbody></table><p>和 <a href="#open-ai">Open AI</a> 类似，获取访问密钥后，将其填入 <code>访问密钥</code> 中即可。</p><h2 id="零一万物" tabindex="-1">零一万物 <a class="header-anchor" href="#零一万物" aria-label="Permalink to &quot;零一万物&quot;">​</a></h2><table tabindex="0"><thead><tr><th>文档</th><th><a href="https://platform.lingyiwanwu.com/docs" target="_blank" rel="noreferrer">https://platform.lingyiwanwu.com/docs</a></th></tr></thead><tbody><tr><td>API 令牌</td><td><a href="https://platform.lingyiwanwu.com/apikeys" target="_blank" rel="noreferrer">https://platform.lingyiwanwu.com/apikeys</a></td></tr></tbody></table><p>和 <a href="#open-ai">Open AI</a> 类似，获取访问密钥后，将其填入 <code>访问密钥</code> 中即可。</p><h2 id="deepseek" tabindex="-1">DeepSeek <a class="header-anchor" href="#deepseek" aria-label="Permalink to &quot;DeepSeek&quot;">​</a></h2><table tabindex="0"><thead><tr><th>文档</th><th><a href="https://platform.deepseek.com/api-docs/zh-cn" target="_blank" rel="noreferrer">https://platform.deepseek.com/api-docs/zh-cn</a></th></tr></thead><tbody><tr><td>API 令牌</td><td><a href="https://platform.deepseek.com/api_keys" target="_blank" rel="noreferrer">https://platform.deepseek.com/api_keys</a></td></tr></tbody></table><p>和 <a href="#open-ai">Open AI</a> 类似，获取访问密钥后，将其填入 <code>访问密钥</code> 中即可。</p><h2 id="通义千问" tabindex="-1">通义千问 <a class="header-anchor" href="#通义千问" aria-label="Permalink to &quot;通义千问&quot;">​</a></h2><table tabindex="0"><thead><tr><th>文档</th><th><a href="https://help.aliyun.com/zh/dashscope/developer-reference/activate-dashscope-and-create-an-api-key" target="_blank" rel="noreferrer">https://help.aliyun.com/zh/dashscope/developer-reference/activate-dashscope-and-create-an-api-key</a></th></tr></thead><tbody><tr><td>API 令牌</td><td><a href="https://dashscope.console.aliyun.com/apiKey" target="_blank" rel="noreferrer">https://dashscope.console.aliyun.com/apiKey</a></td></tr></tbody></table><p>通义千问是阿里公开的大语言模型，它托管在阿里云的灵积平台，所以这要求你注册并开通阿里灵积服务。</p><p>阿里灵积本身是有自己的 API 接口和数据结构的，但是比较好的一点是，阿里灵积同时提供了 Open AI 适配接口，哔哩助理在这里直接使用了阿里灵积提供的适配接口，即：</p><p><code>https://dashscope.aliyuncs.com/compatible-mode/v1</code></p><p>所以它支持的模型是有限的，具体参考这篇文档：<a href="https://help.aliyun.com/zh/dashscope/developer-reference/compatibility-of-openai-with-dashscope/" target="_blank" rel="noreferrer">OpenAI接口兼容</a></p><h2 id="文心一言" tabindex="-1">文心一言 <a class="header-anchor" href="#文心一言" aria-label="Permalink to &quot;文心一言&quot;">​</a></h2><table tabindex="0"><thead><tr><th>文档</th><th><a href="https://cloud.baidu.com/doc/WENXINWORKSHOP/s/flfmc9do2" target="_blank" rel="noreferrer">https://cloud.baidu.com/doc/WENXINWORKSHOP/s/flfmc9do2</a></th></tr></thead><tbody><tr><td>API 令牌</td><td><a href="https://console.bce.baidu.com/qianfan/ais/console/applicationConsole/application" target="_blank" rel="noreferrer">https://console.bce.baidu.com/qianfan/ais/console/applicationConsole/application</a></td></tr></tbody></table><p>文心一言是百度提供的大语言模型，它托管在百度云的千帆大模型平台。它有着自己的托管步骤，甚至对于不同的模型需要单独开通服务，这一点请在其文档中查阅。</p><p>在千帆大模型平台创建应用后，将 <code>API Key</code> 填入 <code>API 密钥（API Key）</code>，将 <code>Secret Key</code> 填入 <code>密文密钥（Secret Key）</code> 即可。</p><h3 id="自定义模型" tabindex="-1">自定义模型 <a class="header-anchor" href="#自定义模型" aria-label="Permalink to &quot;自定义模型&quot;">​</a></h3><p>千帆大模型平台支持非常多常见的大语言模型，如果你打算使用的模型不在预定义模型列表中，请在千帆平台的模型列表内确认模型 ID，然后在应用内创建自定义模型。</p><p>比如你想要使用 <code>Meta-Llama-3-8B</code> 这个模型，那么就可以按照以下步骤来操作：</p><ol><li><p>在千帆模型平台控制台的 <a href="https://console.bce.baidu.com/qianfan/ais/console/onlineService" target="_blank" rel="noreferrer">在线服务</a> 板块中找到 <code>Meta-Llama-3-8B</code>，从提示显示的服务地址可知，其模型 ID 是 <code>llama_3_8b</code>。</p><div style="max-width:300px;"><p><img src="'+p+'" alt="千帆 Meta-Llama-3-8B"></p></div></li><li><p>根据模型 ID 创建自定义模型</p><div style="max-width:300px;"><p><img src="'+d+`" alt="自定义聊天模型"></p></div></li></ol><h2 id="腾讯混元" tabindex="-1">腾讯混元 <a class="header-anchor" href="#腾讯混元" aria-label="Permalink to &quot;腾讯混元&quot;">​</a></h2><table tabindex="0"><thead><tr><th>文档</th><th><a href="https://cloud.tencent.com/document/product/1729/105701" target="_blank" rel="noreferrer">https://cloud.tencent.com/document/product/1729/105701</a></th></tr></thead><tbody><tr><td>API 令牌</td><td><a href="https://console.cloud.tencent.com/cam/capi" target="_blank" rel="noreferrer">https://console.cloud.tencent.com/cam/capi</a></td></tr></tbody></table><p>混元大模型托管在腾讯云上，你需要在 <a href="https://console.cloud.tencent.com/cam/capi" target="_blank" rel="noreferrer">API 密钥管理</a> 中创建一个密钥。</p><p>需要注意的是，由于安全限制，密钥的 <code>Secret Key</code> 仅在创建密钥时可见。</p><p>在应用中，设置项的对应关系是：</p><ul><li><code>Secret Key</code> -&gt; <code>密文密钥（Secret Key）</code></li><li><code>Secret Id</code> -&gt; <code>密文 ID（Secret ID）</code></li></ul><h2 id="讯飞星火" tabindex="-1">讯飞星火 <a class="header-anchor" href="#讯飞星火" aria-label="Permalink to &quot;讯飞星火&quot;">​</a></h2><table tabindex="0"><thead><tr><th>文档</th><th><a href="https://www.xfyun.cn/doc/spark/Web.html" target="_blank" rel="noreferrer">https://www.xfyun.cn/doc/spark/Web.html</a></th></tr></thead><tbody><tr><td>API 令牌</td><td><a href="https://console.xfyun.cn/services/bm35" target="_blank" rel="noreferrer">https://console.xfyun.cn/services/bm35</a></td></tr></tbody></table><p>星火大模型是科大讯飞推出的大语言模型，在使用该模型家族前，你需要在 <a href="https://www.xfyun.cn/" target="_blank" rel="noreferrer">讯飞开放平台</a> 先注册一个应用，然后开通对应的模型服务。</p><p>未开通的模型将无法使用。</p><p>注册后，你能在 <code>星火认知大模型</code> 的模型页中找到 <code>服务接口认证信息</code>，这些是你需要填入应用中的设置项，具体对应关系如下：</p><ul><li><code>APPID</code> -&gt; <code>应用 ID</code></li><li><code>APISecret</code> -&gt; <code>密文（Secret）</code></li><li><code>APIKey</code> -&gt; <code>API 密钥（API Key）</code></li></ul><h2 id="字节豆包" tabindex="-1">字节豆包 <a class="header-anchor" href="#字节豆包" aria-label="Permalink to &quot;字节豆包&quot;">​</a></h2><table tabindex="0"><thead><tr><th>文档</th><th><a href="https://www.volcengine.com/docs/82379/1263482" target="_blank" rel="noreferrer">https://www.volcengine.com/docs/82379/1263482</a></th></tr></thead><tbody><tr><td>API 令牌</td><td><a href="https://console.volcengine.com/ark/region:ark+cn-beijing/apiKey" target="_blank" rel="noreferrer">https://console.volcengine.com/ark/region:ark+cn-beijing/apiKey</a></td></tr></tbody></table><p>豆包大模型是字节跳动推出的大语言模型，在使用该模型前，你需要在 <a href="https://www.volcengine.com/" target="_blank" rel="noreferrer">火山引擎</a> 注册账户，然后在火山方舟的模型推理页面 <a href="https://www.volcengine.com/docs/82379/1099522" target="_blank" rel="noreferrer">创建推理接入点</a>.</p><p>你必须手动部署模型接入点，火山引擎没有提供预设的推理接入点。</p><p>之后，你需要在 <a href="https://console.volcengine.com/ark/region:ark+cn-beijing/apiKey" target="_blank" rel="noreferrer">API Key 管理</a> 中创建一个 API Key，将其填入应用中的设置项。</p><h3 id="自定义模型-1" tabindex="-1">自定义模型 <a class="header-anchor" href="#自定义模型-1" aria-label="Permalink to &quot;自定义模型&quot;">​</a></h3><p>请参照 <a href="https://www.volcengine.com/docs/82379/1099522" target="_blank" rel="noreferrer">创建推理接入点</a> 先部署模型。</p><p>然后在应用中点击创建自定义模型，将推理接入点 ID（ep-xxxxxxx）粘贴进模型 ID，然后给自定义模型取一个易读的名称，这样就创建好了一个自定义模型，然后就可以开始和这个模型聊天了。</p><h2 id="silicon-flow" tabindex="-1">Silicon Flow <a class="header-anchor" href="#silicon-flow" aria-label="Permalink to &quot;Silicon Flow&quot;">​</a></h2><table tabindex="0"><thead><tr><th>文档</th><th><a href="https://openrouter.ai/docs/quick-start" target="_blank" rel="noreferrer">https://openrouter.ai/docs/quick-start</a></th></tr></thead><tbody><tr><td>API 令牌</td><td><a href="https://docs.siliconflow.cn/docs/4-api%E8%B0%83%E7%94%A8" target="_blank" rel="noreferrer">https://docs.siliconflow.cn/docs/4-api调用</a></td></tr></tbody></table><p>和 <a href="#open-ai">Open AI</a> 类似，获取访问密钥后，将其填入 <code>访问密钥</code> 中即可。</p><h2 id="openrouter" tabindex="-1">OpenRouter <a class="header-anchor" href="#openrouter" aria-label="Permalink to &quot;OpenRouter&quot;">​</a></h2><table tabindex="0"><thead><tr><th>文档</th><th><a href="https://openrouter.ai/docs/quick-start" target="_blank" rel="noreferrer">https://openrouter.ai/docs/quick-start</a></th></tr></thead><tbody><tr><td>API 令牌</td><td><a href="https://openrouter.ai/keys" target="_blank" rel="noreferrer">https://openrouter.ai/keys</a></td></tr></tbody></table><p>和 <a href="#open-ai">Open AI</a> 类似，获取访问密钥后，将其填入 <code>访问密钥</code> 中即可。</p><h2 id="together-ai" tabindex="-1">Together AI <a class="header-anchor" href="#together-ai" aria-label="Permalink to &quot;Together AI&quot;">​</a></h2><table tabindex="0"><thead><tr><th>文档</th><th><a href="https://docs.together.ai/docs/quickstart" target="_blank" rel="noreferrer">https://docs.together.ai/docs/quickstart</a></th></tr></thead><tbody><tr><td>API 令牌</td><td><a href="https://api.together.xyz/settings/api-keys" target="_blank" rel="noreferrer">https://api.together.xyz/settings/api-keys</a></td></tr></tbody></table><p>和 <a href="#open-ai">Open AI</a> 类似，获取访问密钥后，将其填入 <code>访问密钥</code> 中即可。</p><h2 id="groq" tabindex="-1">Groq <a class="header-anchor" href="#groq" aria-label="Permalink to &quot;Groq&quot;">​</a></h2><table tabindex="0"><thead><tr><th>文档</th><th><a href="https://console.groq.com/docs/quickstart" target="_blank" rel="noreferrer">https://console.groq.com/docs/quickstart</a></th></tr></thead><tbody><tr><td>API 令牌</td><td><a href="https://console.groq.com/keys" target="_blank" rel="noreferrer">https://console.groq.com/keys</a></td></tr></tbody></table><p>和 <a href="#open-ai">Open AI</a> 类似，获取访问密钥后，将其填入 <code>访问密钥</code> 中即可。</p><h2 id="perplexity" tabindex="-1">Perplexity <a class="header-anchor" href="#perplexity" aria-label="Permalink to &quot;Perplexity&quot;">​</a></h2><table tabindex="0"><thead><tr><th>文档</th><th><a href="https://docs.perplexity.ai/docs/getting-started" target="_blank" rel="noreferrer">https://docs.perplexity.ai/docs/getting-started</a></th></tr></thead><tbody><tr><td>API 令牌</td><td><a href="https://www.perplexity.ai/settings/api" target="_blank" rel="noreferrer">https://www.perplexity.ai/settings/api</a></td></tr></tbody></table><p>和 <a href="#open-ai">Open AI</a> 类似，获取访问密钥后，将其填入 <code>访问密钥</code> 中即可。</p><h2 id="mistral-ai" tabindex="-1">Mistral AI <a class="header-anchor" href="#mistral-ai" aria-label="Permalink to &quot;Mistral AI&quot;">​</a></h2><table tabindex="0"><thead><tr><th>文档</th><th><a href="https://docs.mistral.ai/" target="_blank" rel="noreferrer">https://docs.mistral.ai/</a></th></tr></thead><tbody><tr><td>API 令牌</td><td><a href="https://console.mistral.ai/api-keys/" target="_blank" rel="noreferrer">https://console.mistral.ai/api-keys/</a></td></tr></tbody></table><p>和 <a href="#open-ai">Open AI</a> 类似，获取访问密钥后，将其填入 <code>访问密钥</code> 中即可。</p><h2 id="ollama" tabindex="-1">Ollama <a class="header-anchor" href="#ollama" aria-label="Permalink to &quot;Ollama&quot;">​</a></h2><table tabindex="0"><thead><tr><th>文档</th><th><a href="https://github.com/ollama/ollama/blob/main/README.md#quickstart" target="_blank" rel="noreferrer">https://github.com/ollama/ollama/blob/main/README.md#quickstart</a></th></tr></thead><tbody><tr><td>服务地址</td><td>默认为 <code>http://localhost:11434/v1</code></td></tr><tr><td>API 令牌</td><td>默认为 <code>ollama</code></td></tr></tbody></table><p>Ollama 是一个热门的本地模型托管服务，它自身的使用已经足够写一篇万余字的文档，这里不再赘述，请自行学习。</p><p>哔哩助理支持以 API 接口的形式访问 Ollama，Ollama 提供了 Open AI 适配接口，使得哔哩助理可以简单地接入。</p><p>相应的，由于是在本地运行大模型，应用没有内置任何初始模型，你需要根据当前 ollama 已经拉取到本地的模型创建自定义模型。</p><h3 id="创建自定义模型" tabindex="-1">创建自定义模型 <a class="header-anchor" href="#创建自定义模型" aria-label="Permalink to &quot;创建自定义模型&quot;">​</a></h3><p>假设你刚刚安装 ollama，并打算运行 <code>Qwen 2</code>（通义千问 2） 大模型，你需要按如下步骤操作：</p><ol><li><p>通过 ollama 拉取模型到本地（这里以 <a href="https://ollama.com/library/qwen2:0.5b" target="_blank" rel="noreferrer">qwen2 0.5b</a> 举例）</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">ollama</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> pull</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> qwen2:0.5b</span></span></code></pre></div></li><li><p>调用以下命令确认模型已经下载到本地：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">ollama</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> list</span></span></code></pre></div><p>它应该返回类似下面的内容：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">NAME</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">                    ID</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">              SIZE</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    MODIFIED</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">qwen2:0.5b</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">              6f48b936a09f</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">    352</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> MB</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  36</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> seconds</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> ago</span></span></code></pre></div></li><li><p>在应用中根据模型名称创建自定义模型：</p><div style="max-width:300px;"><p><img src="`+h+'" alt="自定义聊天模型"></p></div></li></ol>',114)]))}const A=a(s,[["render",c]]);export{I as __pageData,A as default};
