import{_ as o,c as t,a0 as a,o as l}from"./chunks/framework.CoDf8mVe.js";const n="/assets/chat-tool-overflyout.D7yC3fd_.png",i="/assets/tool-model.DQmvL7gP.png",s="/assets/tool-call-flyout.B4BhCzcn.png",r="/assets/chat-weatcher.BrPbqjRL.png",v=JSON.parse('{"title":"Tool Usage","description":"","frontmatter":{},"headers":[],"relativePath":"en/chat-tools.md","filePath":"en/chat-tools.md","lastUpdated":null}'),p={name:"en/chat-tools.md"};function h(c,e,d,u,m,g){return l(),t("div",null,e[0]||(e[0]=[a('<h1 id="tool-usage" tabindex="-1">Tool Usage <a class="header-anchor" href="#tool-usage" aria-label="Permalink to &quot;Tool Usage&quot;">​</a></h1><p>Rodel Agent supports a standard tool invocation process:</p><ol><li>Send the prompt along with the toolset definition to the large language model.</li><li>The model determines whether to use the tool based on the prompt. <ul><li>If needed, it returns the tool name. <ol><li>Invoke the specified tool (function).</li><li>Return the result to the model.</li><li>The model integrates the prompt and the tool invocation result to provide a response.</li></ol></li><li>If not needed, it directly generates a response.</li></ul></li><li>The application displays the final model response.</li></ol><p>For example, you know that large language models typically do not have internet access and cannot answer real-time questions.</p><p>For example, what is the weather like in a certain city today?</p><p>However, the model can call a specified tool to obtain the latest weather information and then return the answer you need based on the weather information.</p><h2 id="import-tool-plugins" tabindex="-1">Import Tool Plugins <a class="header-anchor" href="#import-tool-plugins" aria-label="Permalink to &quot;Import Tool Plugins&quot;">​</a></h2><p>In the Rodel Agent chat interface, you can find the import plugin feature in the overflow menu at the top of the left panel.</p><div style="max-width:360px;"><p><img src="'+n+'" alt="Tool Invocation Pop-up"></p></div><p>The plugin package for Rodel Agent is essentially a zip archive. For details on how to create one, please refer to <a href="./tool-dev">Plugin Development</a>.</p><p>After importing, a Plugins folder will be created in your working directory, containing the extracted plugin files.</p><h2 id="model-support" tabindex="-1">Model Support <a class="header-anchor" href="#model-support" aria-label="Permalink to &quot;Model Support&quot;">​</a></h2><p>Tool invocation requires model support. For example, in Open AI, GPT-3.5 Turbo supports tool invocation, but GPT-4 Vision Preview does not.</p><p>To check if a model supports tool invocation, ensure that the model has a tool icon.</p><div style="max-width:240px;"><p><img src="'+i+'" alt="Tool Invocation Icon"></p></div><p>For custom models, if you are sure that the model supports tool invocation, you can enable the tool invocation feature when creating it.</p><h2 id="using-tools" tabindex="-1">Using Tools <a class="header-anchor" href="#using-tools" aria-label="Permalink to &quot;Using Tools&quot;">​</a></h2><p>In the chat interface, you need to switch to a model that supports tool invocation first, then click the button in the toolbar and check the plugins you need to use.</p><div style="max-width:240px;"><p><img src="'+s+'" alt="Tool Invocation Selection"></p></div><div class="tip custom-block github-alert"><p class="custom-block-title">TIP</p><p>In principle, you should not check too many plugins, as each plugin often contains multiple methods, and the descriptions of each method will also occupy the context window length.</p></div><p>Once the plugins are selected and the model supports tool invocation, you can start asking questions.</p><p>Assume your tool supports querying weather information. You can then ask questions like &quot;What is the weather like in XXX today?&quot;, and the model will automatically call the tool to provide an answer.</p><div style="max-width:500px;"><p><img src="'+r+'" alt="Weather Q&amp;A"></p></div>',23)]))}const y=o(p,[["render",h]]);export{v as __pageData,y as default};
